[{"authors":["nirbhay"],"categories":null,"content":"I am a PhD candidate advised by Prof. Dhruv Batra in the School of Interactive Computing within the College of Computing at Georgia Tech. I graduated from IIT Kanpur in 2017 with a bachelor\u0026rsquo;s degree in Computer Science, where I worked with Prof. Piyush Rai.\nMy long term goal is to develop algorithms that are capable of intelligent and flexible decision making. I seek to build artificial agents capable of learning from limited data, reasoning under uncertainty and with guarantees of generalization to novel scenarios. My research covers the areas of theoretical and application based reinforcement learning, information theory and bayesian inference.\nI also enjoy speed solving, hiking and powerlifting. My official World Cube Association record for fastest single solve of the 3 x 3 Rubik\u0026rsquo;s Cube is 15.33 seconds.\nYou can find my resume here.\n","date":1629840673,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1643405473,"objectID":"49d0cb3898140a0bc59e0b08b749ae05","permalink":"https://nirbhayjm.github.io/author/nirbhay-modhe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nirbhay-modhe/","section":"authors","summary":"I am a PhD candidate advised by Prof. Dhruv Batra in the School of Interactive Computing within the College of Computing at Georgia Tech. I graduated from IIT Kanpur in 2017 with a bachelor\u0026rsquo;s degree in Computer Science, where I worked with Prof.","tags":null,"title":"Nirbhay Modhe","type":"authors"},{"authors":["**Nirbhay Modhe**","[Harish Kamath](http://hkamath.com/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ashwin Kalyan](http://ashwinkalyan.com/)"],"categories":[],"content":"","date":1629840673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643405473,"objectID":"ff0b76a8ad94d0ad1ae22d276bd56dad","permalink":"https://nirbhayjm.github.io/publication/mambrl/","publishdate":"2021-08-24T17:31:13-04:00","relpermalink":"/publication/mambrl/","section":"publication","summary":"This work shows that value-aware model learning, known for its numerous theoretical benefits, is also practically viable for solving challenging continuous control tasks in prevalent model-based reinforcement learning algorithms. First, we derive a novel value-aware model learning objective by bounding the model-advantage i.e. model performance difference, between two MDPs or models given a fixed policy, achieving superior performance to prior value-aware objectives in most continuous control environments. Second, we identify the issue of stale value estimates in naively substituting value-aware objectives in place of maximum-likelihood in dyna-style model-based RL algorithms. Our proposed remedy to this issue bridges the long-standing gap in theory and practice of value-aware model learning by enabling successful deployment of all value-aware objectives in solving several continuous control robotic manipulation and locomotion tasks. Our results are obtained with minimal modifications to two popular and open-source model-based RL algorithms -- SLBO and MBPO, without tuning any existing hyper-parameters, while also demonstrating better performance of value-aware objectives than these baseline in some environments.","tags":["reinforcement learning","model based RL","value equivalence","value aware model learning"],"title":"Model-Advantage and Value-Aware Models for Model-Based Reinforcement Learning: Bridging the Gap in Theory and Practice","type":"publication"},{"authors":["**Nirbhay Modhe**\u0026ast;","[Harish Kamath\u0026ast;](http://hkamath.com/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ashwin Kalyan](http://ashwinkalyan.com/)"],"categories":[],"content":"","date":1593120673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593120673,"objectID":"2733cff9e5b6e9c5bb66354625505679","permalink":"https://nirbhayjm.github.io/publication/model-adv/","publishdate":"2020-06-25T17:31:13-04:00","relpermalink":"/publication/model-adv/","section":"publication","summary":"Despite the breakthroughs achieved by Reinforcement Learning (RL) in recent years, RL agents often fail to perform well in unseen environments. This inability to generalize to new environments prevents their deployment in the real world. To help measure this gap in performance, we introduce model-advantage - a quantity similar to the well-known (policy) advantage function. First, we show relationships between the proposed model-advantage and generalization in RL â€” using which we provide guarantees on the gap in performance of an agent in new environments. Further, we conduct toy experiments to show that even a sub-optimal policy (learnt with minimal interactions with the target environment) can help predict if a training environment (say, a simulator) helps learn policies that generalize. We then show connections with Model Based RL.","tags":["reinforcement learning","model based RL","value equivalence","value aware model learning"],"title":"Bridging Worlds in Reinforcement Learning with Model-Advantage","type":"publication"},{"authors":["**Nirbhay Modhe**","[Prithvijit Chattopadhyay](https://prithv1.xyz/)","[Mohit Sharma](https://ms-sharma.github.io/)","[Abhishek Das](https://abhishekdas.com/)","[Devi Parikh](https://www.cc.gatech.edu/~parikh/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ramakrishna Vedantam](https://vrama91.github.io/)"],"categories":[],"content":"","date":1587763873,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587763873,"objectID":"b2c143798445efed1ee66100589e2873","permalink":"https://nirbhayjm.github.io/publication/ir-vic/","publishdate":"2020-04-24T17:31:13-04:00","relpermalink":"/publication/ir-vic/","section":"publication","summary":"We propose a novel framework to identify sub-goals useful for exploration in sequential decision making tasks under partial observability. We utilize the variational intrinsic control framework (Gregor et al., 2016) which maximizes empowerment -- the ability to reliably reach a diverse set of states and show how to identify sub-goals as states with high necessary option information through an information theoretic regularizer. Despite being discovered without explicit goal supervision, our sub-goals provide better exploration and sample complexity on challenging grid-world navigation tasks compared to supervised counterparts in prior work.","tags":["reinforcement learning"],"title":"IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL","type":"publication"},{"authors":["[Vikas Jain\u0026ast;](https://github.com/vikasjiitk)","**Nirbhay Modhe**\u0026ast;","[Piyush Rai](http://www.cse.iitk.ac.in/users/piyush/)"],"categories":[],"content":"","date":1493069473,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587763873,"objectID":"90aa6c1a3c0ca26fd0d687e91b094cd3","permalink":"https://nirbhayjm.github.io/publication/gen-eml/","publishdate":"2017-04-24T17:31:13-04:00","relpermalink":"/publication/gen-eml/","section":"publication","summary":"We present a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example. Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted least-square regression problems, each of which can be solved easily, efficiently, and in parallel. Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for large data sets, even when using moderate computational resources. We report both quantitative and qualitative results for our framework on several benchmark data sets, comparing it with a number of state-of-the-art methods.\"","tags":["multilabel learning"],"title":"Scalable Generative Models for Multi-label Learning with Missing Labels","type":"publication"}]