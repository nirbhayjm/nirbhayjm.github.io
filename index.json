[{"authors":["nirbhay"],"categories":null,"content":"I am a Post-Doctoral Fellow at the Center for Data Science in the Nell Hodgson Woodruff School of Nursing at Emory University, working with Prof. Xiao Hu. Prior to this, I have received my PhD in Computer Science from the School of Interactive Computing within the College of Computing at Georgia Tech, where I was advised by Prof. Dhruv Batra. I have received my bachelor\u0026rsquo;s degree in Computer Science from IIT Kanpur, where I worked with Prof. Piyush Rai.\nMy long term goal is to develop machine learning and reinforcement learning algorithms for biomedical data analysis and sequential decision making in medical treatment.\nI also enjoy powerlifting, hiking, and speed solving. My official World Cube Association record for fastest single solve of the 3 x 3 Rubik\u0026rsquo;s Cube is 15.33 seconds.\nYou can find my resume here.\n","date":1664141473,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1664141473,"objectID":"49d0cb3898140a0bc59e0b08b749ae05","permalink":"https://nirbhayjm.github.io/author/nirbhay-modhe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nirbhay-modhe/","section":"authors","summary":"I am a Post-Doctoral Fellow at the Center for Data Science in the Nell Hodgson Woodruff School of Nursing at Emory University, working with Prof. Xiao Hu. Prior to this, I have received my PhD in Computer Science from the School of Interactive Computing within the College of Computing at Georgia Tech, where I was advised by Prof.","tags":null,"title":"Nirbhay Modhe","type":"authors"},{"authors":["**Nirbhay Modhe**","[Qiaozi Gao](https://scholar.google.com/citations?user=Ub3LlsgAAAAJ\u0026hl=en)","[Ashwin Kalyan](http://ashwinkalyan.com/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Govind Thattai](https://scholar.google.com/citations?user=ZiagaFYAAAAJ\u0026hl=en)","[Gaurav Sukhatme](https://viterbi.usc.edu/directory/faculty/Sukhatme/Gaurav)"],"categories":[],"content":"","date":1664141473,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664141473,"objectID":"a4b747eea8deedc304f76ab26d110fbe","permalink":"https://nirbhayjm.github.io/publication/vaoffrl/","publishdate":"2022-09-25T17:31:13-04:00","relpermalink":"/publication/vaoffrl/","section":"publication","summary":"Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to seen data). We observe improved performance in several offline RL tasks and find that our augmentation strategy consistently leads to overall higher average dataset Q-value estimates than the baseline.","tags":["reinforcement learning","offline RL","offline model-based RL"],"title":"Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations","type":"publication"},{"authors":["**Nirbhay Modhe**","[Harish Kamath](http://hkamath.com/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ashwin Kalyan](http://ashwinkalyan.com/)"],"categories":[],"content":"","date":1629840673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643405473,"objectID":"ff0b76a8ad94d0ad1ae22d276bd56dad","permalink":"https://nirbhayjm.github.io/publication/mambrl/","publishdate":"2021-08-24T17:31:13-04:00","relpermalink":"/publication/mambrl/","section":"publication","summary":"This work shows that value-aware model learning, known for its numerous theoretical benefits, is also practically viable for solving challenging continuous control tasks in prevalent model-based reinforcement learning algorithms. First, we derive a novel value-aware model learning objective by bounding the model-advantage i.e. model performance difference, between two MDPs or models given a fixed policy, achieving superior performance to prior value-aware objectives in most continuous control environments. Second, we identify the issue of stale value estimates in naively substituting value-aware objectives in place of maximum-likelihood in dyna-style model-based RL algorithms. Our proposed remedy to this issue bridges the long-standing gap in theory and practice of value-aware model learning by enabling successful deployment of all value-aware objectives in solving several continuous control robotic manipulation and locomotion tasks. Our results are obtained with minimal modifications to two popular and open-source model-based RL algorithms -- SLBO and MBPO, without tuning any existing hyper-parameters, while also demonstrating better performance of value-aware objectives than these baseline in some environments.","tags":["reinforcement learning","model based RL","value equivalence","value aware model learning"],"title":"Model-Advantage and Value-Aware Models for Model-Based Reinforcement Learning: Bridging the Gap in Theory and Practice","type":"publication"},{"authors":["**Nirbhay Modhe**\u0026ast;","[Harish Kamath\u0026ast;](http://hkamath.com/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ashwin Kalyan](http://ashwinkalyan.com/)"],"categories":[],"content":"","date":1593120673,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593120673,"objectID":"2733cff9e5b6e9c5bb66354625505679","permalink":"https://nirbhayjm.github.io/publication/model-adv/","publishdate":"2020-06-25T17:31:13-04:00","relpermalink":"/publication/model-adv/","section":"publication","summary":"Despite the breakthroughs achieved by Reinforcement Learning (RL) in recent years, RL agents often fail to perform well in unseen environments. This inability to generalize to new environments prevents their deployment in the real world. To help measure this gap in performance, we introduce model-advantage - a quantity similar to the well-known (policy) advantage function. First, we show relationships between the proposed model-advantage and generalization in RL â€” using which we provide guarantees on the gap in performance of an agent in new environments. Further, we conduct toy experiments to show that even a sub-optimal policy (learnt with minimal interactions with the target environment) can help predict if a training environment (say, a simulator) helps learn policies that generalize. We then show connections with Model Based RL.","tags":["reinforcement learning","model based RL","value equivalence","value aware model learning"],"title":"Bridging Worlds in Reinforcement Learning with Model-Advantage","type":"publication"},{"authors":["**Nirbhay Modhe**","[Prithvijit Chattopadhyay](https://prithv1.xyz/)","[Mohit Sharma](https://ms-sharma.github.io/)","[Abhishek Das](https://abhishekdas.com/)","[Devi Parikh](https://www.cc.gatech.edu/~parikh/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ramakrishna Vedantam](https://vrama91.github.io/)"],"categories":[],"content":"","date":1587763873,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587763873,"objectID":"b2c143798445efed1ee66100589e2873","permalink":"https://nirbhayjm.github.io/publication/ir-vic/","publishdate":"2020-04-24T17:31:13-04:00","relpermalink":"/publication/ir-vic/","section":"publication","summary":"We propose a novel framework to identify sub-goals useful for exploration in sequential decision making tasks under partial observability. We utilize the variational intrinsic control framework (Gregor et al., 2016) which maximizes empowerment -- the ability to reliably reach a diverse set of states and show how to identify sub-goals as states with high necessary option information through an information theoretic regularizer. Despite being discovered without explicit goal supervision, our sub-goals provide better exploration and sample complexity on challenging grid-world navigation tasks compared to supervised counterparts in prior work.","tags":["reinforcement learning"],"title":"IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL","type":"publication"},{"authors":["[Vikas Jain\u0026ast;](https://github.com/vikasjiitk)","**Nirbhay Modhe**\u0026ast;","[Piyush Rai](http://www.cse.iitk.ac.in/users/piyush/)"],"categories":[],"content":"","date":1493069473,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587763873,"objectID":"90aa6c1a3c0ca26fd0d687e91b094cd3","permalink":"https://nirbhayjm.github.io/publication/gen-eml/","publishdate":"2017-04-24T17:31:13-04:00","relpermalink":"/publication/gen-eml/","section":"publication","summary":"We present a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example. Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted least-square regression problems, each of which can be solved easily, efficiently, and in parallel. Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for large data sets, even when using moderate computational resources. We report both quantitative and qualitative results for our framework on several benchmark data sets, comparing it with a number of state-of-the-art methods.\"","tags":["multilabel learning"],"title":"Scalable Generative Models for Multi-label Learning with Missing Labels","type":"publication"}]