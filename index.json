[{"authors":["nirbhay"],"categories":null,"content":"I am a PhD student advised by Prof. Dhruv Batra in the School of Interactive Computing within the College of Computing at Georgia Tech. I graduated from IIT Kanpur in 2017 with a bachelor\u0026rsquo;s degree in Computer Science, where I worked with Prof. Piyush Rai.\nMy long term goal is to develop algorithms that are capable of intelligent and flexible decision making. I seek to build artificial agents capable of learning from limited data, reasoning under uncertainty and with guarantees of generalization to novel scenarios. My research covers the areas of theoretical and application based reinforcement learning, information theory and bayesian inference.\nI also enjoy speed solving, hiking and powerlifting. My official World Cube Association record for fastest single solve of the 3 x 3 Rubik\u0026rsquo;s Cube is 15.33 seconds.\nYou can find my resume here.\n","date":1611523873,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1587763873,"objectID":"49d0cb3898140a0bc59e0b08b749ae05","permalink":"https://nirbhayjm.github.io/author/nirbhay-modhe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nirbhay-modhe/","section":"authors","summary":"I am a PhD student advised by Prof. Dhruv Batra in the School of Interactive Computing within the College of Computing at Georgia Tech. I graduated from IIT Kanpur in 2017 with a bachelor\u0026rsquo;s degree in Computer Science, where I worked with Prof.","tags":null,"title":"Nirbhay Modhe","type":"authors"},{"authors":["**Nirbhay Modhe**","[Harish Kamath](http://hkamath.com/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ashwin Kalyan](http://ashwinkalyan.com/)"],"categories":[],"content":"","date":1611523873,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587418273,"objectID":"2733cff9e5b6e9c5bb66354625505679","permalink":"https://nirbhayjm.github.io/publication/model-adv/","publishdate":"2021-01-24T17:31:13-04:00","relpermalink":"/publication/model-adv/","section":"publication","summary":"Model-based Reinforcement Learning (MBRL) algorithms have been traditionally designed with the goal of learning accurate dynamics of the environment. This introduces a mismatch between the objectives of model-learning and the overall learning problem of finding an optimal policy. Value-aware model learning, an alternative model-learning paradigm to maximum likelihood, proposes to inform model-learning through the value function of the learnt policy. While this paradigm is theoretically sound, it does not scale beyond toy settings. In this work, we propose a novel value-aware objective that is an upper bound on the absolute performance difference of a policy across two models. Further, we propose a general purpose algorithm that modifies the standard MBRL pipeline -- enabling learning with value aware objectives. Our proposed objective, in conjunction with this algorithm, is the first successful instantiation of value-aware MBRL on challenging continuous control environments, outperforming previous value-aware objectives and with competitive performance w.r.t. MLE-based MBRL approaches.","tags":["reinforcement learning","model based RL","value equivalence","value aware model learning"],"title":"Model-Advantage Optimization for Model-Based Reinforcement Learning","type":"publication"},{"authors":["**Nirbhay Modhe**","[Prithvijit Chattopadhyay](https://prithv1.xyz/)","[Mohit Sharma](https://ms-sharma.github.io/)","[Abhishek Das](https://abhishekdas.com/)","[Devi Parikh](https://www.cc.gatech.edu/~parikh/)","[Dhruv Batra](https://www.cc.gatech.edu/~dbatra/index.html)","[Ramakrishna Vedantam](https://vrama91.github.io/)"],"categories":[],"content":"","date":1587763873,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587763873,"objectID":"b2c143798445efed1ee66100589e2873","permalink":"https://nirbhayjm.github.io/publication/ir-vic/","publishdate":"2020-04-24T17:31:13-04:00","relpermalink":"/publication/ir-vic/","section":"publication","summary":"We propose a novel framework to identify sub-goals useful for exploration in sequential decision making tasks under partial observability. We utilize the variational intrinsic control framework (Gregor this http URL., 2016) which maximizes empowerment -- the ability to reliably reach a diverse set of states and show how to identify sub-goals as states with high necessary option information through an information theoretic regularizer. Despite being discovered without explicit goal supervision, our sub-goals provide better exploration and sample complexity on challenging grid-world navigation tasks compared to supervised counterparts in prior work.","tags":["reinforcement learning"],"title":"IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL","type":"publication"},{"authors":["[Vikas Jain\u0026ast;](https://github.com/vikasjiitk)","**Nirbhay Modhe**\u0026ast;","[Piyush Rai](http://www.cse.iitk.ac.in/users/piyush/)"],"categories":[],"content":"","date":1493069473,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587763873,"objectID":"90aa6c1a3c0ca26fd0d687e91b094cd3","permalink":"https://nirbhayjm.github.io/publication/gen-eml/","publishdate":"2017-04-24T17:31:13-04:00","relpermalink":"/publication/gen-eml/","section":"publication","summary":"We present a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example. Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted least-square regression problems, each of which can be solved easily, efficiently, and in parallel. Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for large data sets, even when using moderate computational resources. We report both quantitative and qualitative results for our framework on several benchmark data sets, comparing it with a number of state-of-the-art methods.\"","tags":["multilabel learning"],"title":"Scalable Generative Models for Multi-label Learning with Missing Labels","type":"publication"}]