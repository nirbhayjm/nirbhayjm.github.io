<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.52" />
  <meta name="author" content="Nirbhay Modhe">
  <meta name="description" content="PhD Student">

  <link rel="stylesheet" href="/css/highlight.min.css">
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="https://nirbhayjm.github.io/index.xml" type="application/rss+xml" title="Nirbhay Modhe">
  <link rel="feed" href="https://nirbhayjm.github.io/index.xml" type="application/rss+xml" title="Nirbhay Modhe">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://nirbhayjm.github.io/publication/unsup-ds-arxiv/">

  

  <title>Unsupervised Discovery of Decision States for Transfer in Reinforcement Learning | Nirbhay Modhe</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Nirbhay Modhe</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        
        <li class="nav-item">
          <a href="/#about">
            Home
          </a>
        </li>
        
        <li class="nav-item">
          <a href="/#news">
            News
          </a>
        </li>
        
        <li class="nav-item">
          <a href="/#publications_selected">
            Publications
          </a>
        </li>
        
        <li class="nav-item">
          <a href="/#teaching">
            Teaching
          </a>
        </li>
        
        <li class="nav-item">
          <a href="/#contact">
            Contact
          </a>
        </li>
        
      </ul>

    </div>
  </div>
</nav>

<div class="container">

  <div class="pub" itemscope itemtype="http://schema.org/CreativeWork">
    <div class="pub-title">
      <h1 itemprop="name">Unsupervised Discovery of Decision States for Transfer in Reinforcement Learning</h1>
      <span class="pub-authors" itemprop="author">
                
                <strong>Nirbhay Modhe</strong>, Prithvijit Chattopadhyay, Mohit Sharma, Abhishek Das, Devi Parikh, Dhruv Batra, Ramakrishna Vedantam
                
            </span>

    </div>

    

    
    <h3>Abstract</h3>
    <p class="pub-abstract" itemprop="text">We present a hierarchical reinforcement learning (HRL) or options framework for identifying decision states. Informally speaking, these are states considered important by the agent&rsquo;s policy e.g. , for navigation, decision states would be crossroads or doors where an agent needs to make strategic decisions. While previous work (most notably Goyal et. al., 2019) discovers decision states in a task/goal specific (or ‘supervised’) manner, we do so in a goal-independent (or ‘unsupervised’) manner, i.e. entirely without any goal or extrinsic rewards. Our approach combines two hitherto disparate ideas -  1) <em>intrinsic control</em> (Gregor et. al., 2016, Eysenbach et. al., 2018): learning a set of options that allow an agent to reliably reach a diverse set of states, and 2) information bottleneck (Tishby et. al., 2000): penalizing mutual information between the option $\Omega$ and the states $s_t$ visited in the trajectory. The former encourages an agent to reliably explore the environment; the latter allows identification of decision states as the ones with high mutual information $I(\Omega; a_t | s_t)$ despite the bottleneck. Our results demonstrate that 1) our model learns interpretable decision states in an unsupervised manner, and 2) these learned decision states transfer to goal-driven tasks in new environments, effectively guide exploration, and improve performance.</p>
    

    <div class="row">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div>
          <div class="col-xs-12 col-sm-9">Preprint (Under Review)</div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>

    <div class="row">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading">Date</div>
          <div class="col-xs-12 col-sm-9" itemprop="datePublished">
            July, 2019
          </div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>


    <div class="visible-xs space-below"></div>

    <div class="space-below"></div>

    <div class="article-style"><!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. -->
</div>

  </div>

  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://nirbhayjm.github.io/publication/gen-eml/"><span
      aria-hidden="true">&larr;</span> Scalable Generative Models for Multi-label Learning with Missing Labels</a></li>
    

    
  </ul>
</nav>


</div>
<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2016 Nirbhay Modhe &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/isotope.pkgd.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-88127819-1', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>
    

    
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
    

  </body>
</html>

